{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIXSNkWEflbqDTeukCaGua"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09fZxN1rPfG1","executionInfo":{"status":"ok","timestamp":1710945822269,"user_tz":-330,"elapsed":10461,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"ba741399-5b4b-43fc-f68a-8cff0ad08ec3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-rake\n","  Downloading python_rake-1.5.0-py3-none-any.whl (14 kB)\n","Installing collected packages: python-rake\n","Successfully installed python-rake-1.5.0\n"]}],"source":["pip install python-rake\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"2Jodom5hNxXe"}},{"cell_type":"code","source":["pip install rake_nltk\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8zZHaRbQT2g","executionInfo":{"status":"ok","timestamp":1711088074896,"user_tz":-330,"elapsed":8423,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"de84939e-c8d7-4af5-dcc9-be56c8345e4c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rake_nltk\n","  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n","Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake_nltk) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.66.2)\n","Installing collected packages: rake_nltk\n","Successfully installed rake_nltk-1.0.6\n"]}]},{"cell_type":"code","source":[" import nltk\n"," nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rxny_bFzQkgk","executionInfo":{"status":"ok","timestamp":1710945833842,"user_tz":-330,"elapsed":3067,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"d6189834-01c1-44a1-e4c3-fdc54133ffd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":815},"id":"F93s-0WCQsZU","executionInfo":{"status":"error","timestamp":1707471988881,"user_tz":-330,"elapsed":440,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"013b75e0-576f-4d4a-e822-a30557aa8749"},"execution_count":null,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-041bd0a4726d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Extract keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keywords_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get the ranked keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rake_nltk/rake.py\u001b[0m in \u001b[0;36mextract_keywords_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mText\u001b[0m \u001b[0mto\u001b[0m \u001b[0mextract\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_text_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keywords_from_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rake_nltk/rake.py\u001b[0m in \u001b[0;36m_tokenize_text_to_sentences\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mper\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tokenize_sentence_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"]}]},{"cell_type":"code","source":["from rake_nltk import Rake\n","\n","# Sample text\n","text = \"Natural language processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language.\"\n","\n","# Create a Rake object\n","r = Rake()\n","\n","# Extract keywords\n","r.extract_keywords_from_text(text)\n","\n","# Get the ranked keywords\n","keywords = r.get_ranked_phrases()\n","\n","# Print the keywords\n","for keyword in keywords:\n","    print(keyword)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":799},"id":"uBNqo4uQQ5t7","executionInfo":{"status":"error","timestamp":1708147989311,"user_tz":-330,"elapsed":987,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"6a0fbebb-2f53-4bc0-b2a9-23c4871a40dc"},"execution_count":null,"outputs":[{"output_type":"error","ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-041bd0a4726d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Extract keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keywords_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get the ranked keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rake_nltk/rake.py\u001b[0m in \u001b[0;36mextract_keywords_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mText\u001b[0m \u001b[0mto\u001b[0m \u001b[0mextract\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_text_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_keywords_from_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rake_nltk/rake.py\u001b[0m in \u001b[0;36m_tokenize_text_to_sentences\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mper\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tokenize_sentence_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"]}]},{"cell_type":"code","source":["from rake_nltk import Rake\n","\n","def generate_title(paragraph):\n","    # Create a Rake object\n","    r = Rake()\n","\n","    # Extract keywords from the paragraph\n","    r.extract_keywords_from_text(paragraph)\n","\n","    # Get the ranked keywords\n","    keywords = r.get_ranked_phrases()\n","\n","    # Take the top N keywords to form the title (adjust N as needed)\n","    N = 3\n","    title = ' '.join(keywords[:N])\n","\n","    return title\n","\n","# Example paragraph\n","paragraph = \"Abraham Benjamin de Villiers (born 17 February 1984) is a South African former international cricketer, and a current commentator. AB de Villiers was named as the ICC ODI Player of the Year[2] three times during his 15-year international career and was one of the five Wisden cricketers of the decade at the end of 2019.[3] He is regarded as one of the greatest cricketers in the history of the sport and as the best batsman of his era.[4][5] de Villiers began his international career as a wicket-keeper-batsman, but he has played most often solely as a batsman. He batted at various positions in the batting order, but predominantly in the middle-order. Regarded as one of the most innovative and destructive batsmen in the modern era, de Villiers is known for a range of unorthodox shots, particularly behind the wicket-keeper.[6] He made his international debut in a Test match against England in 2004 and first played a One Day International (ODI) in early 2005. His debut in Twenty20 International cricket came in 2006. He scored over 8,000 runs in both Test and ODI cricket and is one of the very few batsmen to have a batting average of over fifty in both forms of the game.[7] In limited overs cricket, he is an attacking player.[8] He holds the record for the fastest ODI fifty(16 balls), fastest ODI century(31 balls), and fastest ODI 150(62 balls).\"\n","\n","# Generate a title from the paragraph\n","resulting_title = generate_title(paragraph)\n","\n","# Print the resulting title\n","print(\"Original Paragraph:\")\n","print(paragraph)\n","print(\"\\nGenerated Title:\")\n","print(resulting_title)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LWGGHU9BRMts","executionInfo":{"status":"ok","timestamp":1707473472402,"user_tz":-330,"elapsed":465,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"173d45d2-4576-4ccc-c217-44ba90435c84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Paragraph:\n","Abraham Benjamin de Villiers (born 17 February 1984) is a South African former international cricketer, and a current commentator. AB de Villiers was named as the ICC ODI Player of the Year[2] three times during his 15-year international career and was one of the five Wisden cricketers of the decade at the end of 2019.[3] He is regarded as one of the greatest cricketers in the history of the sport and as the best batsman of his era.[4][5] de Villiers began his international career as a wicket-keeper-batsman, but he has played most often solely as a batsman. He batted at various positions in the batting order, but predominantly in the middle-order. Regarded as one of the most innovative and destructive batsmen in the modern era, de Villiers is known for a range of unorthodox shots, particularly behind the wicket-keeper.[6] He made his international debut in a Test match against England in 2004 and first played a One Day International (ODI) in early 2005. His debut in Twenty20 International cricket came in 2006. He scored over 8,000 runs in both Test and ODI cricket and is one of the very few batsmen to have a batting average of over fifty in both forms of the game.[7] In limited overs cricket, he is an attacking player.[8] He holds the record for the fastest ODI fifty(16 balls), fastest ODI century(31 balls), and fastest ODI 150(62 balls).\n","\n","Generated Title:\n","16 balls ), fastest odi century south african former international cricketer born 17 february 1984\n"]}]},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","from transformers import pipeline\n","from rake_nltk import Rake\n","from summarizer import Summarizer\n","import random\n","import nltk\n","from nltk import sent_tokenize, word_tokenize, pos_tag\n","from nltk.corpus import stopwords\n","from collections import Counter\n","import re\n","\n","\n","\n","def convert_to_title(paragraph, model, tokenizer):\n","    # Tokenize and convert paragraph to title\n","    inputs = tokenizer(\"summarize: \" + paragraph, return_tensors=\"pt\", max_length=512, truncation=True)\n","    summary_ids = model.generate(**inputs)\n","    title = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","    return title\n","def generate_summary(text):\n","    summarizer = pipeline(\"summarization\")\n","    summary = summarizer(text, max_length=512, min_length=50, length_penalty=2.0, num_beams=4)\n","    title =summary[0]['summary_text'].strip()\n","    return title\n","\n","def generate_title(paragraph):\n","    # Create a Rake object\n","    r = Rake()\n","\n","    # Extract keywords from the paragraph\n","    r.extract_keywords_from_text(paragraph)\n","\n","    # Get the ranked keywords\n","    keywords = r.get_ranked_phrases()\n","\n","    # Take the top N keywords to form the title (adjust N as needed)\n","    N = 5\n","    title = ' '.join(keywords[:N])\n","\n","    return title\n","\n","def paragraph_to_title(paragraph):\n","    # Load the BERT Extractive Summarizer model\n","    summarizer =Summarizer()\n","\n","    # Summarize the paragraph to generate a title\n","    title = Summarizer(paragraph)\n","\n","    return title\n","\n","\n","def generate_title2(topic, keywords):\n","    title_prefixes = [\"The Ultimate Guide to\", \"Exploring\", \"Mastering\", \"A Comprehensive Look at\"]\n","    title_suffixes = [\": A Deep Dive\", \"Demystified\", \"In-Depth Analysis\", \"Unlocking the Secrets\"]\n","\n","    title = f\"{random.choice(title_prefixes)} {topic} {random.choice(title_suffixes)}\"\n","\n","    if keywords:\n","        keyword_phrase = ', '.join(keywords)\n","        title += f\" ({keyword_phrase})\"\n","\n","    return title\n","\n","\n","\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('stopwords')\n","\n","def generate_title_from_paragraph(user_paragraph):\n","    sentences = sent_tokenize(user_paragraph)\n","    words = [word for sent in sentences for word in word_tokenize(sent) if word.isalpha()]\n","\n","    # Remove common English stop words\n","    stop_words = set(stopwords.words('english'))\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","\n","    # Get key phrases based on parts of speech\n","    tagged_words = pos_tag(filtered_words)\n","    key_phrases = [word for word, pos in tagged_words if pos.startswith('NN') or pos.startswith('VB')]\n","\n","    # Construct the title\n","    title = generate_title2(\" \".join(key_phrases), [])\n","\n","    return title\n","\n","\n","\n","#new 5th headingor title\n","\n","def generate_heading(paragraph):\n","    # Tokenize paragraph into words\n","    words = re.findall(r'\\b\\w+\\b', paragraph.lower())\n","\n","    # Remove stopwords and punctuation\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word not in stop_words and word.isalpha()]\n","\n","    # Calculate word frequencies\n","    word_freq = Counter(words)\n","\n","    # Choose a random length for the heading between 1 and the length of the paragraph\n","    heading_length = random.randint(5, len(words))\n","\n","    # Select top N most frequent words for heading\n","    heading_words = [word for word, _ in word_freq.most_common(heading_length)]\n","\n","    # Capitalize the first letter of each word in the heading\n","    heading = ' '.join(word.capitalize() for word in heading_words)\n","\n","    return heading\n","\n","def convert_paragraph_to_title(paragraph, heading_length):\n","    # Tokenize paragraph into words\n","    words = re.findall(r'\\b\\w+\\b', paragraph.lower())\n","\n","    # Remove stopwords and punctuation\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word not in stop_words and word.isalpha()]\n","\n","    # Calculate word frequencies\n","    word_freq = Counter(words)\n","\n","    # Select top N most frequent words for title\n","    title_words = [word for word, _ in word_freq.most_common(heading_length)]\n","\n","    # Capitalize the first letter of each word in the title\n","    title = ' '.join(word.capitalize() for word in title_words)\n","\n","    return title\n","#end\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Load pre-trained T5 model and tokenizer\n","    model_name = \"t5-small\"\n","    model = T5ForConditionalGeneration.from_pretrained(model_name)\n","    tokenizer = T5Tokenizer.from_pretrained(model_name)\n","\n","    # User input paragraph\n","    user_paragraph = input(\"Enter the paragraph you want to convert into a title: \")\n","\n","    # Convert user input paragraph to title\n","    title = convert_to_title(user_paragraph, model, tokenizer)\n","    print(\"Converted Title 1 title = : \")\n","    print(title)\n","\n","\n","    # Rake Algo\n","    resulting_title = generate_title(user_paragraph)\n","    print(\"\\Converted Title 2 title = :\")\n","  #  if resulting_title.rfind('.'):\n","   #   last_period_index = resulting_title.rfind('.')\n","    #  if last_period_index != -1:\n","     #   resulting_title = resulting_title[:last_period_index+1]\n","      #  print(resulting_title)\n","      #else:\n","    print(resulting_title)\n","\n","   # print(resulting_title)\n","    #summarizer\n","    result_title = paragraph_to_title(user_paragraph)\n","    print(\"\\nConverted Title 3 title = :\")\n","    print(result_title)\n","    # nltk\n","    title = generate_title_from_paragraph(user_paragraph)\n","    print(\"Converted title 4 : = \")\n","    print(title)\n","\n","#new\n","\n"," #   heading = generate_heading(user_paragraph)\n","  #  print(\"Heading  5 : \", heading)\n","\n","    title = convert_paragraph_to_title(user_paragraph, len(heading.split()))\n","    print(\"Title 5 : \", title)\n","#end\n","    #Pipe line\n","    generated_summary = generate_summary(user_paragraph)\n","    print(\"Generated Summary:\", generated_summary)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_2JUOV31XQtB","executionInfo":{"status":"ok","timestamp":1711090066744,"user_tz":-330,"elapsed":20295,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"e2a8496c-252a-4da4-86c5-54ec02a55008"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["Enter the paragraph you want to convert into a title: Facebook is a social media and social networking service owned by American technology conglomerate Meta Platforms. Created in 2004 by Mark Zuckerberg with four other Harvard College students and roommates Eduardo Saverin, Andrew McCollum, Dustin Moskovitz, and Chris Hughes, its name derives from the face book directories often given to American university students. Membership was initially limited to Harvard students, gradually expanding to other North American universities. Since 2006, Facebook allows everyone to register from 13 years old (or older), except in the case of a handful of nations, where the age limit is 14 years.[6] As of December 2022, Facebook claimed 3 billion monthly active users.[7] As of October 2023, Facebook ranked as the 3rd most visited website in the world, with 22.56% of its traffic coming from the United States.[8][9] It was the most downloaded mobile app of the 2010s\n"]},{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"stream","name":"stdout","text":["Converted Title 1 title = : \n","facebook is owned by the u.s. technology conglomerate Meta Platforms.\n","\\Converted Title 2 title = :\n","facebook claimed 3 billion monthly active users face book directories often given american technology conglomerate meta platforms social networking service owned north american universities\n","\n","Converted Title 3 title = :\n","<summarizer.summarizer.Summarizer object at 0x7e5c5a31a980>\n","Converted title 4 : = \n","The Ultimate Guide to Facebook media service owned American technology conglomerate Meta Platforms Created Mark Zuckerberg Harvard College students roommates Eduardo Saverin Andrew McCollum Dustin Moskovitz Chris Hughes name derives face book directories given American university students Membership limited Harvard students expanding universities Facebook allows everyone register years case nations age limit years December Facebook claimed users October Facebook ranked visited world traffic coming United States downloaded app : A Deep Dive\n","Title 5 :  Facebook American Students Social Harvard Years Media Networking Service Owned Technology Conglomerate Meta Platforms Created Mark Zuckerberg Four College Roommates\n"]},{"output_type":"stream","name":"stderr","text":["Your max_length is set to 512, but your input_length is only 185. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=92)\n"]},{"output_type":"stream","name":"stdout","text":["Generated Summary: Facebook is a social media and social networking service owned by Meta Platforms . Its name derives from the face book directories often given to American university students . Membership was initially limited to Harvard students, gradually expanding to other North American universities . As of December 2022, Facebook claimed 3 billion monthly active users .\n"]}]},{"cell_type":"code","source":["from nltk import sent_tokenize, word_tokenize, pos_tag\n","from nltk.corpus import stopwords\n","\n","def generate_title_from_paragraph(user_paragraph):\n","    sentences = sent_tokenize(user_paragraph)\n","    words = [word for sent in sentences for word in word_tokenize(sent) if word.isalpha()]\n","\n","    # Remove common English stop words\n","    stop_words = set(stopwords.words('english'))\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","\n","    # Get key phrases based on parts of speech\n","    tagged_words = pos_tag(filtered_words)\n","    key_phrases = [word for word, pos in tagged_words if pos.startswith('NN') or pos.startswith('VB')]\n","\n","    # Construct the title\n","    title = generate_title2(\" \".join(key_phrases), [])\n","\n","    return title\n","\n","if __name__ == \"__main__\":\n","   user_paragraph = input(\"Enter the paragraph you want to convert into a title: \")\n","\n","   title = generate_title_from_paragraph(user_paragraph)\n","   print(title)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43Nqp9xZeRdg","executionInfo":{"status":"ok","timestamp":1709052874381,"user_tz":-330,"elapsed":22734,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"83a0984d-7b31-4de8-adba-76cf459667de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the paragraph you want to convert into a title: Paris Whitney Hilton (born February 17, 1981)[3][4] is an American media personality, businesswoman, and socialite. Born in New York City, and raised there and in Los Angeles, she is a great-granddaughter of Conrad Hilton, the founder of Hilton Hotels. Hilton first attracted tabloid attention in the late 1990s, when she became a fixture in NYC's social scene, and ventured into modeling at age 19, signing with Donald Trump's agency Trump Model Management. After David LaChapelle photographed her and sister Nicky for the September 2000 issue of Vanity Fair, Hilton was proclaimed \"New York's leading It Girl\" in 2001.[3] The reality television series The Simple Life (2003–2007), in which she co-starred with her friend Nicole Richie, and a leaked 2003 sex tape with her then-boyfriend Rick Salomon, later released as 1 Night in Paris (2004), catapulted her to global fame.\n","Exploring Paris Whitney Hilton born February media personality businesswoman socialite Born New York City raised Los Angeles Conrad Hilton founder Hilton Hotels Hilton attracted attention became NYC scene ventured age signing Donald Trump agency Trump Model Management David LaChapelle photographed sister Nicky September issue Vanity Fair Hilton proclaimed New York leading Girl reality television series Simple Life friend Nicole Richie leaked sex tape Rick Salomon released Night Paris catapulted fame Unlocking the Secrets\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"focbOjr2eNJq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install nltk\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAqqjsDQYQJK","executionInfo":{"status":"ok","timestamp":1710945840309,"user_tz":-330,"elapsed":6478,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"92adeb85-9e9a-4a01-a180-014ac58e3e95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"]}]},{"cell_type":"code","source":["from gensim.summarization import summarize\n","\n","text = \"Your long piece of text goes here.\"\n","summary = summarize(text)\n","print(summary)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403},"id":"LduNKO8jWLjy","executionInfo":{"status":"error","timestamp":1707926458705,"user_tz":-330,"elapsed":12,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"f03d4e5b-0508-49c2-dad7-db2952660aa1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'gensim.summarization'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-15040f7a584a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Your long piece of text goes here.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h6qGVyDQWUGD","executionInfo":{"status":"ok","timestamp":1707926453379,"user_tz":-330,"elapsed":9192,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"b9fbebdc-f376-40b4-e4f0-682ebb1be723"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":["import random\n","\n","def randomize_paragraph(paragraph):\n","    words = paragraph.split()\n","    random.shuffle(words)\n","    randomized_paragraph = ' '.join(words)\n","    return randomized_paragraph\n","\n","# Example usage:\n","original_paragraph = \"De Villiers had a reputation as an outstanding fielder, typified by a diving run-out of Simon Katich of Australia in 2006, when he dived to stop the ball, and while still lying on his stomach facing away from the stumps, he tossed the ball backwards over his shoulder and effected a direct hit . His fielding positions other than wicket-keeper were first and second slip and cover ..\"\n","randomized_paragraph = randomize_paragraph(original_paragraph)\n","\n","print(\"Original Paragraph:\")\n","print(original_paragraph)\n","print(\"\\nRandomized Paragraph:\")\n","print(randomized_paragraph)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlR9JqbZTrqa","executionInfo":{"status":"ok","timestamp":1707925806640,"user_tz":-330,"elapsed":418,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"3e366719-0043-490b-8ee1-72384269bf3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Paragraph:\n","De Villiers had a reputation as an outstanding fielder, typified by a diving run-out of Simon Katich of Australia in 2006, when he dived to stop the ball, and while still lying on his stomach facing away from the stumps, he tossed the ball backwards over his shoulder and effected a direct hit . His fielding positions other than wicket-keeper were first and second slip and cover ..\n","\n","Randomized Paragraph:\n","while he the first and backwards and facing 2006, from had stumps, stomach His tossed were fielder, typified stop a his positions shoulder cover reputation and Australia in over slip on Simon still other a as direct lying dived fielding than the a when of second hit he his away Katich diving De and the . an to .. of ball outstanding effected run-out wicket-keeper Villiers by ball,\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sj0dFlC-PruI","executionInfo":{"status":"ok","timestamp":1710945841487,"user_tz":-330,"elapsed":514,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"b767ce75-1348-432d-f2b0-a44bce997816"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCff8BhGPc0y","executionInfo":{"status":"ok","timestamp":1710945841488,"user_tz":-330,"elapsed":6,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"52f56297-473f-4acb-d4e2-0d1f67aba7ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["pip install summarizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXkOOtIiO23N","executionInfo":{"status":"ok","timestamp":1711088114572,"user_tz":-330,"elapsed":17158,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"710d02f6-2036-458c-84fc-bc2b8562852a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting summarizer\n","  Downloading summarizer-0.0.7.tar.gz (280 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.1/280.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from summarizer) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->summarizer) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->summarizer) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->summarizer) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->summarizer) (4.66.2)\n","Building wheels for collected packages: summarizer\n","  Building wheel for summarizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for summarizer: filename=summarizer-0.0.7-py2.py3-none-any.whl size=284209 sha256=126ebb685b4ab8fc5bc20cc6deb4ceddc33574cec654944bbee1a09d03e767ca\n","  Stored in directory: /root/.cache/pip/wheels/20/bb/2d/1fe057c2f729818a5f28c312c3667e8b9d5cfd4af4a39895e7\n","Successfully built summarizer\n","Installing collected packages: summarizer\n","Successfully installed summarizer-0.0.7\n"]}]},{"cell_type":"code","source":["pip install rake_nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jauo833sOWeG","executionInfo":{"status":"ok","timestamp":1710945859116,"user_tz":-330,"elapsed":6114,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"5c4d557e-9f04-4d42-e96a-7101b8404f24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rake_nltk in /usr/local/lib/python3.10/dist-packages (1.0.6)\n","Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake_nltk) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.66.2)\n"]}]},{"cell_type":"code","source":["from gensim import corpora\n","from gensim.models import LdaModel\n","from gensim.test.utils import common_texts\n","\n","# Create a dictionary representation of the documents\n","dictionary = corpora.Dictionary(common_texts)\n","\n","# Create a bag-of-words corpus\n","corpus = [dictionary.doc2bow(text) for text in common_texts]\n","\n","# Train the LDA model\n","lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary)\n","\n","# Print the topics\n","print(lda_model.print_topics())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KVcv1oNYbvoS","executionInfo":{"status":"ok","timestamp":1708148286265,"user_tz":-330,"elapsed":671,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"4649df78-3e82-49b4-e06e-291cda60258d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"]},{"output_type":"stream","name":"stdout","text":["[(0, '0.087*\"trees\" + 0.084*\"user\" + 0.084*\"graph\" + 0.083*\"human\" + 0.083*\"minors\" + 0.083*\"response\" + 0.083*\"computer\" + 0.083*\"system\" + 0.083*\"survey\" + 0.083*\"eps\"'), (1, '0.238*\"graph\" + 0.164*\"trees\" + 0.164*\"minors\" + 0.090*\"interface\" + 0.089*\"human\" + 0.089*\"computer\" + 0.089*\"survey\" + 0.015*\"user\" + 0.015*\"system\" + 0.015*\"response\"'), (2, '0.142*\"user\" + 0.142*\"system\" + 0.142*\"survey\" + 0.142*\"computer\" + 0.142*\"response\" + 0.142*\"time\" + 0.026*\"trees\" + 0.024*\"human\" + 0.024*\"graph\" + 0.024*\"minors\"'), (3, '0.307*\"system\" + 0.211*\"eps\" + 0.115*\"interface\" + 0.115*\"user\" + 0.115*\"human\" + 0.020*\"trees\" + 0.020*\"graph\" + 0.020*\"minors\" + 0.020*\"survey\" + 0.020*\"response\"'), (4, '0.187*\"time\" + 0.187*\"user\" + 0.186*\"response\" + 0.184*\"trees\" + 0.032*\"graph\" + 0.032*\"minors\" + 0.032*\"human\" + 0.032*\"system\" + 0.032*\"survey\" + 0.032*\"computer\"')]\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize, sent_tokenize\n","from gensim.summarization import keywords\n","\n","def generate_title(paragraph):\n","    # Tokenize the paragraph into sentences\n","    sentences = sent_tokenize(paragraph)\n","\n","    # Extract keywords from each sentence\n","    sentence_keywords = [keywords(sentence, words=5, lemmatize=True).split() for sentence in sentences]\n","\n","    # Flatten the list of lists into a single list of keywords\n","    all_keywords = [word for sublist in sentence_keywords for word in sublist]\n","\n","    # Take the top N keywords to form the title (adjust N as needed)\n","    N = 5\n","    title_keywords = ' '.join(all_keywords[:N])\n","\n","    return title_keywords\n","\n","# Example paragraph\n","paragraph = \"Natural language processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans using natural language. It involves the development of algorithms and models that enable computers to understand, interpret, and generate human-like language.\"\n","\n","# Generate a title from the paragraph\n","resulting_title = generate_title(paragraph)\n","\n","# Print the resulting title\n","print(\"Original Paragraph:\")\n","print(paragraph)\n","print(\"\\nGenerated Title:\")\n","print(resulting_title)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403},"id":"nELsayxccHC_","executionInfo":{"status":"error","timestamp":1708148305501,"user_tz":-330,"elapsed":589,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"50d0760c-c61b-4b44-d37b-096aa985eb9e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'gensim.summarization'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-dc0978b11772>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Tokenize the paragraph into sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["pip install keywords\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXDWWB0Ylahe","executionInfo":{"status":"ok","timestamp":1708148536681,"user_tz":-330,"elapsed":9958,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"1e1ae297-dfd1-482d-b96d-51c1ac687074"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keywords\n","  Downloading keywords-0.0.1.zip (3.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: keywords\n","  Building wheel for keywords (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keywords: filename=keywords-0.0.1-py3-none-any.whl size=3193 sha256=e248c94193d152d2be4d12d2c1e53cc6ad38c13dccfdc854a74055ef6cb199fb\n","  Stored in directory: /root/.cache/pip/wheels/d0/c8/b2/970db6eecb3eea041715b61e10a26a880c3f40adddd65e11d3\n","Successfully built keywords\n","Installing collected packages: keywords\n","Successfully installed keywords-0.0.1\n"]}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Je6pq6a9lj_X","executionInfo":{"status":"ok","timestamp":1708148692850,"user_tz":-330,"elapsed":10863,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"89100086-54c2-40e6-ad25-7d5a69d51270"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":["# Install the gensim module\n","!pip install gensim\n","\n","# Import the gensim module\n","import gensim\n","\n","# Import the keywords function from the gensim.summarization module\n","from gensim.summarization import keywords\n","\n","# Check if the import was successful\n","print(\"The gensim.summarization module has been imported successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"id":"0dFPg8qimLys","executionInfo":{"status":"error","timestamp":1708148768939,"user_tz":-330,"elapsed":351,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"815ac555-6a31-44e0-d9ef-f929c3241a6b"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'gensim.summarization'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-bbf9620f8fb1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Import the keywords function from the gensim.summarization module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Check if the import was successful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('genism')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o55cpk-ikwFc","executionInfo":{"status":"ok","timestamp":1708148778575,"user_tz":-330,"elapsed":1760,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"8d812a84-41e8-48a3-a0c6-7a87d6725bc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Error loading genism: Package 'genism' not found in index\n"]},{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["pip install nltk word_tokenize\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qj7fOmdrdkGk","executionInfo":{"status":"ok","timestamp":1708148779888,"user_tz":-330,"elapsed":8,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"f673775b-1491-4a48-af9f-b3c4460328d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement word_tokenize (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for word_tokenize\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["pip install nltk gensim\n"],"metadata":{"id":"UziLD67rdqpr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708148447616,"user_tz":-330,"elapsed":5975,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"6b53cdcc-5d2e-422f-a6c3-cd8816da5d78"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMzMnUZXQ0fr","executionInfo":{"status":"ok","timestamp":1709874179480,"user_tz":-330,"elapsed":10,"user":{"displayName":"Altohid Sheikh","userId":"12609206018277234900"}},"outputId":"f6ee78b6-435f-4573-c38d-87e0cc3c0314"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}]}]}